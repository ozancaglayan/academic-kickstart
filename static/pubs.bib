@inproceedings{caglayan2013humanoid,
 abstract = {In this study we implemented a low-cost, single EEG channel brain computer interface (BCI) running on an embedded computer. The BCI uses steady-state visual evoked potentials (SSVEP) to control the motion of the Kondo KHR-3HV humanoid robot. The subject attends to one of the flickering LEDs attached to the arms of the robot in order to move an arm. SSVEPs are identified by a simple spectrum analysis of the EEG signal.},
 address = {California, USA},
 author = {Caglayan, Ozan and Arslan, Reis Burak},
 booktitle = {BCI Meeting 2013},
 month = {June},
 title = {Humanoid Robot Control with SSVEP on Embedded System},
 url = {http://dx.doi.org/10.3217/978-3-85125-260-6-129},
 year = {2013}
}

@article{caglayan2016multimodal,
 abstract = {The attention mechanism is an important part of the neural machine translation (NMT) where it was reported to produce richer source representation compared to fixed-length encoding sequence-to-sequence models. Recently, the effectiveness of attention has also been explored in the context of image captioning. In this work, we assess the feasibility of a multimodal attention mechanism that simultaneously focus over an image and its natural language description for generating a description in another language. We train several variants of our proposed attention mechanism on the Multi30k multilingual image captioning dataset. We show that a dedicated attention for each modality achieves up to 1.6 points in BLEU and METEOR compared to a textual NMT baseline.},
 author = {Caglayan, Ozan and Barrault, Loïc and Bougares, Fethi},
 journal = {arXiv preprint arXiv:1609.03976},
 month = {September},
 title = {Multimodal Attention for Neural Machine Translation},
 url = {https://arxiv.org/pdf/1609.03976},
 year = {2016}
}

@article{caglayan2017nmtpy,
 abstract = {In this paper, we present nmtpy, a flexible Python toolkit based on Theano for training Neural Machine Translation and other neural sequence-to-sequence architectures. nmtpy decouples the specification of a network from the training and inference utilities to simplify the addition of a new architecture and reduce the amount of boilerplate code to be written. nmtpy has been used for LIUM’s top-ranked submissions to WMT Multimodal Machine Translation and News Translation tasks in 2016 and 2017.},
 author = {Caglayan, Ozan and García-Martínez, Mercedes and Bardet, Adrien and Aransa, Walid and Bougares, Fethi and Barrault, Loïc},
 journal = {The Prague Bulletin of Mathematical Linguistics},
 month = {September},
 number = {1},
 pages = {15--28},
 publisher = {De Gruyter Open},
 title = {NMTPY: A Flexible Toolkit for Advanced Neural Machine Translation Systems},
 url = {https://ufal.mff.cuni.cz/pbml/109/art-caglayan-et-al.pdf},
 volume = {109},
 year = {2017}
}

@inproceedings{caglayan2019multimodal,
 abstract = {Humans are capable of processing speech by making use of multiple sensory modalities. For example, the environment where a conversation takes place generally provides semantic and/or acoustic context that helps us to resolve ambiguities or to recall named entities. Motivated by this, there have been many works studying the integration of visual information into the speech recognition pipeline. Specifically, in our previous work, we propose a multistep visual adaptive training approach which improves the accuracy of an audio-based Automatic Speech Recognition (ASR) system. This approach, however, is not end-to-end as it requires fine-tuning the whole model with an adaptation layer. In this paper, we propose novel end-to-end multimodal ASR systems and compare them to the adaptive approach by using a range of visual representations obtained from state-of-the-art convolutional neural networks. We show that adaptive training is effective for S2S models leading to an absolute improvement of 1.4% in word error rate. As for the end-to-end systems, although they perform better than baseline, the improvements are slightly less than adaptive training, 0.8 absolute WER reduction in single-best models. Using ensemble decoding, end-to-end models reach a WER of 15% which is the lowest score among all systems.},
 author = {Caglayan, Ozan and Sanabria, Ramon and Palaskar, Shruti and Barrault, Loïc and Metze, Florian},
 booktitle = {ICASSP 2019},
 month = {May},
 title = {Multimodal Grounding for Sequence-to-Sequence Speech Recognition},
 url = {https://arxiv.org/pdf/1811.03865},
 year = {2019}
}

@inproceedings{caglayan-EtAl:2016:WMT,
 abstract = {This paper presents the systems developed by LIUM and CVC for the WMT16 Multimodal Machine Translation challenge. We explored various comparative methods, namely phrase-based systems and attentional recurrent neural networks models trained using monomodal or multimodal data. We also performed a human evaluation in order to estimate the usefulness of multimodal data for human machine translation and image description generation. Our systems obtained the best results for both tasks according to the automatic evaluation metrics BLEU and METEOR.},
 address = {Berlin, Germany},
 author = {Caglayan, Ozan  and  Aransa, Walid  and  Wang, Yaxing  and  Masana, Marc  and  García-Martínez, Mercedes  and  Bougares, Fethi  and  Barrault, Loïc  and  van de Weijer, Joost},
 booktitle = {Proceedings of the First Conference on Machine Translation},
 month = {August},
 pages = {627--633},
 publisher = {Association for Computational Linguistics},
 title = {Does Multimodality Help Human and Machine for Translation and Image Captioning?},
 url = {https://arxiv.org/pdf/1605.09186},
 year = {2016}
}

@inproceedings{caglayan-EtAl:2017:WMT,
 abstract = {This paper describes the monomodal and multimodal Neural Machine Translation systems developed by LIUM and CVC for WMT17 Shared Task on Multimodal Translation. We mainly explored two multimodal architectures where either global visual features or convolutional feature maps are integrated in order to benefit from visual context. Our final systems ranked first for both En-De and En-Fr language pairs according to the automatic evaluation metrics METEOR and BLEU.},
 address = {Copenhagen, Denmark},
 author = {Caglayan, Ozan  and  Aransa, Walid  and  Bardet, Adrien  and  García-Martínez, Mercedes  and  Bougares, Fethi  and  Barrault, Loïc  and  Masana, Marc  and  Herranz, Luis  and  van de Weijer, Joost},
 booktitle = {Proceedings of the Second Conference on Machine Translation, Volume 2: Shared Task Papers},
 month = {September},
 pages = {432--439},
 publisher = {Association for Computational Linguistics},
 title = {LIUM-CVC Submissions for WMT17 Multimodal Translation Task},
 url = {https://arxiv.org/pdf/1707.04481},
 year = {2017}
}

@inproceedings{caglayan-EtAl:2018:WMT,
 abstract = {This paper describes the multimodal Neural Machine Translation systems developed by LIUM and CVC for WMT18 Shared Task on Multimodal Translation. This year we propose several modifications to our previous multimodal attention architecture in order to better integrate convolutional features and refine them using encoder-side information. Our final constrained submissions ranked first for English-French and second for English-German language pairs among the constrained submissions according to the automatic evaluation metric METEOR.},
 address = {Belgium, Brussels},
 author = {Caglayan, Ozan  and  Bardet, Adrien  and  Bougares, Fethi  and  Barrault, Loïc and  Wang, Kai  and  Masana, Marc  and  Herranz, Luis  and  van de Weijer, Joost},
 booktitle = {Proceedings of the Third Conference on Machine Translation, Volume 2: Shared Task Papers},
 month = {October},
 pages = {603--608},
 publisher = {Association for Computational Linguistics},
 title = {LIUM-CVC Submissions for WMT18 Multimodal Translation Task},
 url = {https://arxiv.org/pdf/1809.00151},
 year = {2018}
}

@inproceedings{garciamartinez-EtAl:2017:WMT,
 abstract = {This paper describes LIUM submissions to WMT17 News Translation Task for English-German, English-Turkish, English-Czech and English-Latvian language pairs. We train BPE-based attentive Neural Machine Translation systems with and without factored outputs using the open source nmtpy framework. Competitive scores were obtained by ensembling various systems and exploiting the availability of target monolingual corpora for back-translation. The impact of back-translation quantity and quality is also analyzed for English-Turkish where our post-deadline submission surpassed the best entry by +1.6 BLEU.},
 address = {Copenhagen, Denmark},
 author = {García-Martínez, Mercedes  and  Caglayan, Ozan  and  Aransa, Walid  and  Bardet, Adrien  and  Bougares, Fethi  and  Barrault, Loïc},
 booktitle = {Proceedings of the Second Conference on Machine Translation, Volume 2: Shared Task Papers},
 month = {September},
 pages = {288--295},
 publisher = {Association for Computational Linguistics},
 title = {LIUM Machine Translation Systems for WMT17 News Translation Task},
 url = {https://arxiv.org/pdf/1707.04499},
 year = {2017}
}

@inproceedings{caglayan-etal-2019-probing,
    title = "Probing the Need for Visual Context in Multimodal Machine Translation",
    author = {Caglayan, Ozan  and
      Madhyastha, Pranava  and
      Specia, Lucia  and
      Barrault, Lo{\"\i}c},
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/N19-1422",
    pages = "4159--4170",
    abstract = "Current work on multimodal machine translation (MMT) has suggested that the visual modality is either unnecessary or only marginally beneficial. We posit that this is a consequence of the very simple, short and repetitive sentences used in the only available dataset for the task (Multi30K), rendering the source text sufficient as context. In the general case, however, we believe that it is possible to combine visual and textual information in order to ground translations. In this paper we probe the contribution of the visual modality to state-of-the-art MMT models by conducting a systematic analysis where we partially deprive the models from source-side textual context. Our results show that under limited textual context, models are capable of leveraging the visual input to generate better translations. This contradicts the current belief that MMT models disregard the visual modality because of either the quality of the image features or the way they are integrated into the model.",
}

@article{rougier2017sustainable,
 abstract = {Computer science offers a large set of tools for prototyping, writing, running, testing, validating, sharing and reproducing results; however, computational science lags behind. In the best case, authors may provide their source code as a compressed archive and they may feel confident their research is reproducible. But this is not exactly true. James Buckheit and David Donoho proposed more than two decades ago that an article about computational results is advertising, not scholarship. The actual scholarship is the full software environment, code, and data that produced the result. This implies new workflows, in particular in peer-reviews. Existing journals have been slow to adapt: source codes are rarely requested and are hardly ever actually executed to check that they produce the results advertised in the article. ReScience is a peer-reviewed journal that targets computational research and encourages the explicit replication of already published research, promoting new and open-source implementations in order to ensure that the original research can be replicated from its description. To achieve this goal, the whole publishing chain is radically different from other traditional scientific journals. ReScience resides on GitHub where each new implementation of a computational study is made available together with comments, explanations, and software tests.},
 author = {Rougier, Nicolas P and Hinsen, Konrad and Alexandre, Frédéric and Arildsen, Thomas and Barba, Lorena A and Benureau, Fabien CY and Brown, C Titus and De Buyl, Pierre and Caglayan, Ozan and Davison, Andrew P and others},
 journal = {PeerJ Computer Science},
 month = {December},
 pages = {e142},
 publisher = {PeerJ Inc.},
 title = {Sustainable Computational Science: The ReScience Initiative},
 url = {https://doi.org/10.7717/peerj-cs.142},
 volume = {3},
 year = {2017}
}

@inproceedings{sanabria2018how2,
 abstract = {In this paper, we introduce How2, a multimodal collection of instructional videos with English subtitles and crowdsourced Portuguese translations. We also present integrated sequence-to-sequence baselines for machine translation, automatic speech recognition, spoken language translation, and multimodal summarization. By making available data and code for several multimodal natural language tasks, we hope to stimulate more research on these and similar challenges, to obtain a deeper understanding of multimodality in language processing.},
 author = {Sanabria, Ramon and Caglayan, Ozan and Palaskar, Shruti and Elliott, Desmond and Barrault, Loïc and Specia, Lucia and Metze, Florian},
 booktitle = {Proceedings of the Workshop on Visually Grounded Interaction and Language (NeurIPS 2018)},
 month = {November},
 title = {How2: A Large-scale Dataset for Multimodal Language Understanding},
 url = {https://arxiv.org/pdf/1811.00347},
 year = {2018}
}

