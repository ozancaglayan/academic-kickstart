@inproceedings{caglayan-EtAl:2016:WMT,
 abstract = {This paper presents the systems developed by LIUM and CVC for the WMT16 Multimodal Machine Translation challenge. We explored various comparative methods, namely phrase-based systems and attentional recurrent neural networks models trained using monomodal or multimodal data. We also performed a human evaluation in order to estimate the usefulness of multimodal data for human machine translation and image description generation. Our systems obtained the best results for both tasks according to the automatic evaluation metrics BLEU and METEOR.},
 address = {Berlin, Germany},
 author = {Caglayan, Ozan  and  Aransa, Walid  and  Wang, Yaxing  and  Masana, Marc  and  García-Martínez, Mercedes  and  Bougares, Fethi  and  Barrault, Loïc  and  van de Weijer, Joost},
 booktitle = {Proceedings of the First Conference on Machine Translation},
 month = {August},
 pages = {627--633},
 publisher = {Association for Computational Linguistics},
 title = {Does Multimodality Help Human and Machine for Translation and Image Captioning?},
 url = {https://arxiv.org/pdf/1605.09186},
 year = {2016}
}

