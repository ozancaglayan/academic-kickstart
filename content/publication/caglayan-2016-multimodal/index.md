---
title: "Multimodal Attention for Neural Machine Translation"
date: 2016-09-01
publishDate: 2019-06-02T13:17:16.267376Z
authors: ["Ozan Caglayan", "Lo√Øc Barrault", "Fethi Bougares"]
publication_types: ["2"]
abstract: "The attention mechanism is an important part of the neural machine translation (NMT) where it was reported to produce richer source representation compared to fixed-length encoding sequence-to-sequence models. Recently, the effectiveness of attention has also been explored in the context of image captioning. In this work, we assess the feasibility of a multimodal attention mechanism that simultaneously focus over an image and its natural language description for generating a description in another language. We train several variants of our proposed attention mechanism on the Multi30k multilingual image captioning dataset. We show that a dedicated attention for each modality achieves up to 1.6 points in BLEU and METEOR compared to a textual NMT baseline."
featured: false
publication: "*arXiv preprint arXiv:1609.03976*"
url_pdf: "https://arxiv.org/pdf/1609.03976"
---

